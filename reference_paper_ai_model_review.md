ä»¥ä¸‹æ˜¯å®Œæ•´ 36 ç¯‡è«–æ–‡çš„æ•´ç†ç‰ˆè¡¨æ ¼ï¼ˆä¾ä½ æŒ‡å®šçš„å››å¤§åˆ†ç¾¤ï¼‰ã€‚
å¼•ç”¨æ¬¡æ•¸ç‚ºæˆªè‡³ 2025 å¹´æ¨¡å‹è¨˜æ†¶ä¸­ä¹‹å¤§è‡´ä¼°è¨ˆå€¼ã€‚

---

## ğŸ§© æ¨¡å‹çš„èŒƒå¼è®Šé·

| å¹´ä»½   | è«–æ–‡åç¨±                                                            | å‡ºè™•                 | å¼•ç”¨æ¬¡æ•¸ (ä¼°) | æ‘˜è¦                           | ä¸»è¦è²¢ç»            |
| ---- | --------------------------------------------------------------- | ------------------ | -------- | ---------------------------- | --------------- |
| 2004 | Brook for GPUs: Stream Computing on Graphics Hardware           | ACM SIGGRAPH / TOG | â‰ˆ 1.9 k  | å°‡ GPU æŠ½è±¡æˆå¯ç¨‹å¼åŒ–æµè™•ç†å–®å…ƒã€‚          | é–‹å•Ÿ GPGPU æ™‚ä»£ã€‚    |
| 2012 | ImageNet Classification with Deep CNNs (AlexNet)                | NeurIPS 2012       | â‰ˆ 145 k  | GPU è¨“ç·´ CNN çªç ´ ImageNet æº–ç¢ºç‡ã€‚  | æ·±åº¦å­¸ç¿’æ™‚ä»£é–‹ç«¯ã€‚       |
| 2014 | Sequence to Sequence Learning with Neural Networks              | NeurIPS 2014       | â‰ˆ 70 k   | Encoderâ€“Decoder æ¶æ§‹ç¿»è­¯ä»»å‹™ã€‚      | åºåˆ—è½‰æ›åŸºçŸ³ã€‚         |
| 2015 | Distilling the Knowledge in a Neural Network                    | arXiv 1503.02531   | â‰ˆ 17 k   | ä»¥ soft label é€²è¡ŒçŸ¥è­˜è’¸é¤¾ã€‚         | æ¨¡å‹å£“ç¸®å…¸ç¯„ã€‚         |
| 2015 | Deep Residual Learning for Image Recognition (ResNet)           | CVPR 2016          | â‰ˆ 200 k  | æ®˜å·®é€£æ¥è§£æ±ºæ¢¯åº¦æ¶ˆå¤±ã€‚                  | ä½¿è¶…æ·±ç¶²è·¯å¯è¡Œã€‚        |
| 2017 | Attention Is All You Need                                       | NeurIPS 2017       | â‰ˆ 320 k  | å…¨è‡ªæ³¨æ„åŠ›æ¶æ§‹å–ä»£ RNNã€‚               | Transformer èª•ç”Ÿã€‚ |
| 2017 | Mastering the Game of Go without Human Knowledge (AlphaGo Zero) | Nature 2017        | â‰ˆ 25 k   | è‡ªæˆ‘åšå¼ˆ + MCTS å­¸ç¿’æ£‹è—ã€‚            | å¼·åŒ–å­¸ç¿’ç„¡äººç¤ºç¯„å…¸ç¯„ã€‚     |
| 2017 | Outrageously Large Neural Networks (Sparsely-Gated MoE)         | arXiv 1701.06538   | â‰ˆ 3.8 k  | ç¨€ç– Mixture-of-Experts é™ä½è¨ˆç®—é‡ã€‚ | å·¨æ¨¡å‹ç¨€ç–åŒ–é–‹ç«¯ã€‚       |
| 2021 | LoRA: Low-Rank Adaptation of Large Language Models              | arXiv 2106.09685   | â‰ˆ 8 k    | ä½ç§©çŸ©é™£å¾®èª¿ LLMã€‚                  | è¼•é‡åŒ–å¾®èª¿ä¸»æµã€‚        |
| 2022 | Chain-of-Thought Prompting Elicits Reasoning in LLMs            | arXiv 2201.11903   | â‰ˆ 7 k    | å±•ç¤º LLM æ¨ç†æ­¥é©Ÿã€‚                 | æ¨ç†å‹ prompt å¥ åŸºã€‚  |
| 2022 | ReAct: Synergizing Reasoning and Acting in LLMs                 | arXiv 2210.03629   | â‰ˆ 2.5 k  | çµåˆæ¨ç†èˆ‡è¡Œå‹•ã€‚                     | Agent çµæ§‹é››å½¢ã€‚     |

---

## âš™ï¸ Infra èˆ‡è³‡æ–™çš„è®Šé·

| å¹´ä»½          | è«–æ–‡åç¨±                                                             | å‡ºè™•               | å¼•ç”¨æ¬¡æ•¸ (ä¼°) | æ‘˜è¦                  | ä¸»è¦è²¢ç»              |
| ----------- | ---------------------------------------------------------------- | ---------------- | -------- | ------------------- | ----------------- |
| 2018 / 2019 | The Bitter Lesson                                                | R. Sutton Blog   | â‰ˆ 3 k    | ä¸»å¼µè¨ˆç®—å‹æ–¼äººé¡å…ˆé©—ã€‚         | AI é•·æœŸå“²å­¸æŒ‡å¼•ã€‚        |
| 2019        | ZeRO: Memory Optimizations Toward Training Trillion Param Models | arXiv 1910.02054 | â‰ˆ 5.8 k  | åˆ†æ•£å¼æ¢¯åº¦èˆ‡åƒæ•¸åˆ‡åˆ†ã€‚         | æ”¯æ’ DeepSpeedã€‚     |
| 2020        | Scaling Laws for Neural Language Models                          | arXiv 2001.08361 | â‰ˆ 7.4 k  | æ€§èƒ½èˆ‡æ¨¡å‹è¦æ¨¡å‘ˆå†ªå¾‹é—œä¿‚ã€‚       | LLM scaling ç†è«–åŸºç¤ã€‚ |
| 2022        | LAION-5B: Open Large-Scale Image-Text Dataset                    | arXiv 2210.08402 | â‰ˆ 2.2 k  | 58 å„„åœ–æ–‡å° CLIP éæ¿¾ã€‚    | é–‹æºå¤šæ¨¡æ…‹è³‡æ–™é›†é©å‘½ã€‚       |
| 2023        | RefinedWeb: High-Quality Massive Web Corpus                      | arXiv 2306.01116 | â‰ˆ 0.3 k  | æ¸…æ´— Common Crawl èªæ–™ã€‚ | é«˜è³ªæ–‡æœ¬å·¥ç¨‹ç¯„ä¾‹ã€‚         |
| 2024        | MegaScale: Scaling LLM Training to >10 000 GPUs                  | arXiv 2402.15627 | â‰ˆ 0.15 k | è¬å¡è¨“ç·´ç›£æ§èˆ‡å®¹éŒ¯ã€‚          | è¶…å¤§è¦æ¨¡è¨“ç·´å¯¦è¸ã€‚         |

---

## ğŸ—£ èªè¨€æ¨¡å‹çš„ç™¼å±•

| å¹´ä»½   | è«–æ–‡åç¨±                                                                   | å‡ºè™•                 | å¼•ç”¨æ¬¡æ•¸ (ä¼°) | æ‘˜è¦                      | ä¸»è¦è²¢ç»                |
| ---- | ---------------------------------------------------------------------- | ------------------ | -------- | ----------------------- | ------------------- |
| 2013 | Efficient Estimation of Word Representations (Word2Vec)                | arXiv 1301.3781    | â‰ˆ 95 k   | é€£çºŒè©å‘é‡è¡¨å¾µã€‚                | è©åµŒå…¥æ™‚ä»£é–‹å•Ÿã€‚            |
| 2016 | Google Neural Machine Translation (GNMT)                               | arXiv 1609.08144   | â‰ˆ 14 k   | å…¨ seq2seq ç¿»è­¯ç³»çµ±ã€‚         | NMT ç”¢æ¥­åŒ–ã€‚            |
| 2018 | Improving Language Understanding by Generative Pre-Training (GPT-1)    | OpenAI Report      | â‰ˆ 11 k   | ç„¡ç›£ç£é è¨“ç·´ + ç›£ç£å¾®èª¿ã€‚          | é è¨“ç·´ç¯„å¼é–‹ç«¯ã€‚            |
| 2018 | BERT: Pre-training of Deep Bidirectional Transformers                  | NAACL 2019         | â‰ˆ 130 k  | é›™å‘ Transformer MLM é è¨“ç·´ã€‚ | NLP åŸºæº–å…¨é¢è¶…è¶Šã€‚         |
| 2019 | Language Models are Unsupervised Multitask Learners (GPT-2)            | OpenAI Tech Report | â‰ˆ 40 k   | å¤§è¦æ¨¡ Transformer é›¶æ¨£æœ¬æ³›åŒ–ã€‚  | Few/Zero-shot å±•ç¤ºã€‚   |
| 2020 | Language Models are Few-Shot Learners (GPT-3)                          | arXiv 2005.14165   | â‰ˆ 30 k   | 175 B åƒæ•¸å°‘æ¨£æœ¬å­¸ç¿’ã€‚          | Prompt Learning ç¢ºç«‹ã€‚ |
| 2022 | Training LLMs to Follow Instructions with Human Feedback (InstructGPT) | arXiv 2203.02155   | â‰ˆ 8 k    | SFT + RM + PPOã€‚         | RLHF å°é½Šé–‹ç«¯ã€‚          |
| 2024 | TULU 3: Instruction Tuning at Scale                                    | arXiv 2407.15541   | â‰ˆ 50     | SFT + DPO + RLVR å…¨é–‹æºã€‚   | å¾Œè¨“ç·´ pipeline æ–°æ¨™ç«¿ã€‚   |

---

## ğŸ–¼ å¤šæ¨¡æ…‹æ¨¡å‹çš„ç™¼å±•

| å¹´ä»½      | è«–æ–‡åç¨±                                                                            | å‡ºè™•                          | å¼•ç”¨æ¬¡æ•¸ (ä¼°) | æ‘˜è¦                          | ä¸»è¦è²¢ç»                   |
| ------- | ------------------------------------------------------------------------------- | --------------------------- | -------- | --------------------------- | ---------------------- |
| 2014    | Large-Scale Video Classification with CNNs (DeepVideo)                          | CVPR 2014                   | â‰ˆ 5.5 k  | CNN æ‡‰ç”¨æ–¼å½±ç‰‡åˆ†é¡ã€‚                | è¦–è¦ºè·¨å…¥æ™‚åºé ˜åŸŸã€‚              |
| 2014    | Two-Stream CNNs for Action Recognition in Videos                                | NeurIPS 2014                | â‰ˆ 12 k   | å½±åƒèˆ‡å…‰æµé›™è·¯ç¶²è·¯ã€‚                  | å‹•ä½œè¾¨è­˜é‡Œç¨‹ç¢‘ã€‚               |
| 2015    | Generative Adversarial Networks (GAN)                                           | NeurIPS 2014 paper pub 2015 | â‰ˆ 60 k   | ç”Ÿæˆå™¨-åˆ¤åˆ¥å™¨ åšå¼ˆã€‚                 | ç”Ÿæˆå¼æ¨¡å‹é©å‘½ã€‚               |
| 2020    | Denoising Diffusion Probabilistic Models (DDPM)                                 | NeurIPS 2020                | â‰ˆ 23 k   | é€æ­¥å»å™ªç”Ÿæˆå½±åƒã€‚                   | Diffusion ä¸»æµèª•ç”Ÿã€‚        |
| 2020    | An Image is Worth 16Ã—16 Words (ViT)                                             | ICLR 2021                   | â‰ˆ 55 k   | å°‡ Transformer æ‡‰ç”¨æ–¼åœ–åƒã€‚        | Vision Transformer é–‹ç«¯ã€‚ |
| 2021    | CLIP: Learning Transferable Visual Models from Language Supervision             | ICML 2021                   | â‰ˆ 40 k   | æ–‡å­—-åœ–åƒå°æ¯”å­¸ç¿’ã€‚                  | å¤šæ¨¡æ…‹è¡¨å¾µä¸»æµã€‚               |
| 2021-22 | High-Resolution Image Synthesis with Latent Diffusion Models (Stable Diffusion) | arXiv 2112.10752            | â‰ˆ 10 k   | æ½›è®Šé‡ç©ºé–“é«˜æ•ˆç”Ÿæˆã€‚                  | é–‹æ”¾å¼å½±åƒç”Ÿæˆé©å‘½ã€‚             |
| 2022    | DiT: Diffusion Models Beat GANs on Image Synthesis                              | arXiv 2212.09748            | â‰ˆ 2 k    | Transformer ç‚ºéª¨å¹¹çš„ Diffusionã€‚ | çµ±ä¸€ Diffusion æ¶æ§‹ã€‚       |

---

### âœ… æ‘˜è¦æ•´ç†å°çµ

* **æ¨¡å‹çš„èŒƒå¼è®Šé·**ï¼šå¾ GPU â†’ CNN â†’ Transformer â†’ LLM æ¨ç†åŒ–ã€‚
* **Infra èˆ‡è³‡æ–™**ï¼šå¾ ç¡¬é«”èˆ‡åˆ†æ•£è¨“ç·´ åˆ° è³‡æ–™æ²»ç†ã€‚
* **èªè¨€æ¨¡å‹**ï¼šå¾ è©å‘é‡ åˆ° RLHF å°é½Šã€‚
* **å¤šæ¨¡æ…‹**ï¼šå¾ å½±åƒ åˆ° è·¨èªæ„ç”Ÿæˆã€‚

æ˜¯å¦è¦æˆ‘å†ç”Ÿæˆæˆ `.xlsx` æª”ï¼ˆå«å››å€‹ sheet åˆ†ç¾¤ï¼‰æ–¹ä¾¿ä½ å°å…¥ Notion æˆ– Zotero ï¼Ÿ
